{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b30ba56-bd6c-49c6-b88f-a69c50016c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/remote/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/remote/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef save_to_faiss(chunks: list[Document], embeddings = embeddings, FAISS_PATH = FAISS_PATH):\\n    # Clear out the database first.\\n    if os.path.exists(FAISS_PATH):\\n        shutil.rmtree(FAISS_PATH)\\n\\n    # Create a new DB from the documents.\\n    db = FAISS.from_documents(chunks, embeddings)\\n    db.save_local(FAISS_PATH)\\n    print(f\"Saved {len(chunks)} chunks to {FAISS_PATH}.\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "import shutil\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "import nltk\n",
    "import faiss\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "FAISS_PATH = \"faiss\"\n",
    "DATA_PATH = \"ref_db\"\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "def main():\n",
    "    generate_data_store()\n",
    "\n",
    "\n",
    "def generate_data_store():\n",
    "    try:\n",
    "        nxml = load_nxml()\n",
    "        nxchunks = split_text(nxml)\n",
    "        add_to_faiss(nxchunks)\n",
    "    except:\n",
    "        print('nxml error')\n",
    "    documents = load_pdf()\n",
    "    chunks = split_text(documents)\n",
    "    add_to_faiss(chunks)\n",
    "\n",
    "\n",
    "def load_pdf():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=([\"*.pdf\", '*.nxml']), silent_errors = True, show_progress=True, use_multithreading = True, max_concurrency = 12)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def load_nxml():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=('*.nxml'), silent_errors = True, show_progress=True, use_multithreading = True, max_concurrency = 12)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "    \n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=750,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\"\"\"\n",
    "def save_to_faiss(chunks: list[Document], embeddings = embeddings, FAISS_PATH = FAISS_PATH):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(FAISS_PATH):\n",
    "        shutil.rmtree(FAISS_PATH)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = FAISS.from_documents(chunks, embeddings)\n",
    "    db.save_local(FAISS_PATH)\n",
    "    print(f\"Saved {len(chunks)} chunks to {FAISS_PATH}.\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7abbcfce-98ba-4faf-bda0-f25a6663ffd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nxml = load_nxml()\n",
    "#nxchunks = split_text(nxml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f59504-5fc9-48c1-9382-5c47d0285f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef split_list(input_list, chunk_size):\\n    for i in range(0, len(input_list), chunk_size):\\n        yield input_list[i:i + chunk_size]\\n        \\nsplit_docs_chunked = split_list(split_docs, 41000)\\n\\n\\nfor split_docs_chunk in split_docs_chunked:\\n    vectordb = Chroma.from_documents(\\n        documents=split_docs_chunk,\\n        embedding=embeddings,\\n        persist_directory=\\'./chroma_langchain_db,\\n    )\\n    vectordb.persist()\\n\\ndef add_to_faiss(chunks: list[Document], embeddings = embeddings, FAISS_PATH = FAISS_PATH):\\n    vector_store=Chroma(persist_directory=\"./chroma_langchain_db\", embedding_function=embeddings)\\n    uuids = [str(uuid4()) for _ in range(len(chunks))]\\n    \\n    vector_store.add_documents(documents=chunks, ids=uuids)\\n    vector_store.persist()\\n    #vector_store.save_local(FAISS_PATH)\\n    print(f\"Saved {len(chunks)} chunks to {FAISS_PATH}.\")\\n    \\n\\n\\nvector_store = Chroma(\\n    collection_name=\"ref_vector\",\\n    embedding_function=embeddings,\\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\\n)\\nvector_store.persist()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "vector_store.save_local(FAISS_PATH)\n",
    "\n",
    "def add_to_faiss(chunks: list[Document], embeddings = embeddings, FAISS_PATH = FAISS_PATH):\n",
    "\n",
    "    path= FAISS_PATH\n",
    "    vector_store=FAISS.load_local(FAISS_PATH,embeddings, allow_dangerous_deserialization=True)\n",
    "    uuids = [str(uuid4()) for _ in range(len(chunks))]\n",
    "    \n",
    "    vector_store.add_documents(documents=chunks, ids=uuids)\n",
    "    vector_store.save_local(FAISS_PATH)\n",
    "    print(f\"Saved {len(chunks)} chunks to {FAISS_PATH}.\")\n",
    "\"\"\"\n",
    "def split_list(input_list, chunk_size):\n",
    "    for i in range(0, len(input_list), chunk_size):\n",
    "        yield input_list[i:i + chunk_size]\n",
    "        \n",
    "split_docs_chunked = split_list(split_docs, 41000)\n",
    "\n",
    "\n",
    "for split_docs_chunk in split_docs_chunked:\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=split_docs_chunk,\n",
    "        embedding=embeddings,\n",
    "        persist_directory='./chroma_langchain_db,\n",
    "    )\n",
    "    vectordb.persist()\n",
    "\n",
    "def add_to_faiss(chunks: list[Document], embeddings = embeddings, FAISS_PATH = FAISS_PATH):\n",
    "    vector_store=Chroma(persist_directory=\"./chroma_langchain_db\", embedding_function=embeddings)\n",
    "    uuids = [str(uuid4()) for _ in range(len(chunks))]\n",
    "    \n",
    "    vector_store.add_documents(documents=chunks, ids=uuids)\n",
    "    vector_store.persist()\n",
    "    #vector_store.save_local(FAISS_PATH)\n",
    "    print(f\"Saved {len(chunks)} chunks to {FAISS_PATH}.\")\n",
    "    \n",
    "\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"ref_vector\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "vector_store.persist()\n",
    "\"\"\"\n",
    "\n",
    "#add_to_faiss(nxchunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff6ebb5f-8a1f-4ff6-b70c-54e833c79545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                                | 78/14754 [05:06<16:50:31,  4.13s/it]The PDF <_io.BufferedReader name=\"ref_db/hutchinson's clinical methods.pdf\"> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "  4%|███                                                                           | 579/14754 [1:31:50<18:02:19,  4.58s/it]The PDF <_io.BufferedReader name='ref_db/EN_IM_Axioscope_5-7-Vario_V15.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "  7%|█████▍                                                                       | 1052/14754 [5:15:44<51:47:06, 13.61s/it]Error loading file ref_db/Souhami - Oxford Textbook of Oncology .pdf: Unable to get page count.\n",
      "Syntax Warning: May not be a PDF file (continuing anyway)\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't read xref table\n",
      "\n",
      "  8%|█████▉                                                                       | 1140/14754 [5:57:27<80:26:09, 21.27s/it]The PDF <_io.BufferedReader name='ref_db/A Clinical Atlas of Chinese Infants (1996).pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "  8%|██████▌                                                                      | 1253/14754 [7:36:01<34:57:56,  9.32s/it]The PDF <_io.BufferedReader name='ref_db/Clinical communication skills (1st Ed.,1995).pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "  9%|██████▉                                                                     | 1350/14754 [8:35:58<323:26:02, 86.87s/it]The PDF <_io.BufferedReader name='ref_db/Clinical communication skills.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      " 11%|████████                                                                  | 1600/14754 [12:27:43<461:33:43, 126.32s/it]The PDF <_io.BufferedReader name='ref_db/The Patient History Evidence Based Approach (2nd Ed.).pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      " 82%|██████████████████████████████████████████████████████████████▌             | 12154/14754 [13:58:33<2:59:23,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 12153 documents into 1246108 chunks.\n",
      "A multigene panel that includes SLC33A1 and other genes of interest (see Differential Diagnosis) is most likely to identify the genetic cause of the condition while limiting identification of variants of uncertain significance and pathogenic variants in genes that do not explain the underlying phenotype. Note: (1) The genes included in the panel and the diagnostic sensitivity of the testing used for each gene vary by laboratory and are likely to change over time. (2) Some multigene panels may include genes not associated with the condition discussed in this GeneReview. (3) In some laboratories, panel options may include a custom laboratory-designed panel and/or custom phenotype-focused exome analysis that includes genes specified by the clinician. (4) Methods used in a panel may include sequence analysis, deletion/duplication analysis, and/or other non-sequencing-based tests. For an introduction to multigene panels click here. More detailed information for clinicians ordering genetic\n",
      "{'source': 'ref_db/huppke-brendel.pdf', 'start_index': 6699}\n"
     ]
    }
   ],
   "source": [
    "documents = load_pdf()\n",
    "chunks = split_text(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db0c5d9a-7573-446e-bfca-6e062c3b9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.vectorstores import SKLearnVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c3a86cd-a4b2-48dd-b70d-a52d2390a9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chunks.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(chunks, 'short_chunks.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a1a93-fa82-48cd-bb28-84157484e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#vector_store.save_local(FAISS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3978905b-9f6a-4540-8a38-829135e6ffdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
